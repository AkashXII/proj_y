{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c532ec-698e-47d6-95a6-6ec1506cfce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features: 500 users, 9 features\n",
      "Inferred contamination from labels: 0.0200\n",
      "Top-K (for overlap) set to 10 (contamination*N)\n",
      "=== Stability & Calibration Summary ===\n",
      "Seeds used: [0, 1, 2, 3, 5, 7, 11, 13, 17, 19]\n",
      "Samples: 500, features: 9\n",
      "Contamination used: 0.0200, Top-K for overlap: 10\n",
      "Mean Spearman rank correlation across seeds: 0.9828\n",
      "Mean Top-10 Jaccard overlap across seeds: 0.8007\n",
      "Ensemble ROC-AUC (using true labels): 0.0122\n",
      "Best F1 threshold (ensemble percentile): 0.010 -> precision=0.010, recall=0.500, f1=0.020\n",
      "Calibration table saved: calibration_output\\threshold_calibration.csv\n",
      "Per-user stability summary saved: calibration_output\\stability_summary.csv\n",
      "Per-seed CSVs and ensemble file saved in calibration_output/\n",
      "\n",
      "Recommendations:\n",
      "- Ranking is relatively stable across seeds.\n",
      "- Model F1 is low at best threshold. Consider: adding features, ensembling with Autoencoder, or adding rule signals.\n",
      "\n",
      "Next recommended actions:\n",
      "1) Inspect 'stability_summary.csv' and 'ensemble_percentile.csv' to see which users are top-ranked.\n",
      "2) Use ensemble_percentile as the canonical anomaly score for downstream risk aggregation.\n",
      "3) If ranking unstable, run Autoencoder as an orthogonal detector and ensemble its percentiles with IF (average).\n",
      "4) Use the calibration table to pick an operational percentile threshold for alerts (or to map percentile -> risk bands).\n"
     ]
    }
   ],
   "source": [
    "# calibrate_and_stability.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, precision_recall_curve\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ------------- CONFIG --------------\n",
    "FEATURES_CSV = \"features_stress.csv\"          # input features produced earlier\n",
    "RESULTS_CSV = \"anomaly_results_stress.csv\"    # optional; used for explanation fields (not required)\n",
    "OUT_DIR = \"calibration_output\"\n",
    "SEEDS = [0, 1, 2, 3, 5, 7, 11, 13, 17, 19]    # multiple seeds to test stability\n",
    "N_ESTIMATORS = 200\n",
    "# If true labels exist in features CSV, the script will use them; otherwise it runs unsupervised-only checks.\n",
    "# If you know expected anomaly fraction, set this; otherwise we'll infer from labels if present.\n",
    "CONTAMINATION = None   # None -> infer from y_true if available; otherwise set a sensible default like 0.02\n",
    "TOP_K_RULE = None      # None -> compute K from contamination * n_samples\n",
    "# ------------------------------------\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_features(path):\n",
    "    assert os.path.exists(path), f\"{path} not found\"\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    # if is_attack present use it as ground truth\n",
    "    y_true = df[\"is_attack\"].astype(int) if \"is_attack\" in df.columns else None\n",
    "    X = df.drop(columns=[\"is_attack\"]) if \"is_attack\" in df.columns else df.copy()\n",
    "    return X, y_true\n",
    "\n",
    "def train_iforest_get_scores(X, seed, contamination):\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    model = IsolationForest(n_estimators=N_ESTIMATORS, contamination=contamination, random_state=seed)\n",
    "    model.fit(Xs)\n",
    "    # sklearn decision_function: larger => more normal. Convert so larger => more anomalous\n",
    "    scores = -model.decision_function(Xs)   # higher = more anomalous\n",
    "    preds = model.predict(Xs)               # -1 anomaly, 1 normal\n",
    "    y_pred = np.where(preds == -1, 1, 0)\n",
    "    return scores, y_pred, model, scaler\n",
    "\n",
    "def percentile_from_scores(scores):\n",
    "    # compute percentile rank so final score is in [0,1] with 1 = most anomalous\n",
    "    ranks = scores.argsort().argsort()  # 0..N-1\n",
    "    # we want percentile = rank / (N-1)\n",
    "    if len(scores) <= 1:\n",
    "        return np.ones_like(scores)\n",
    "    perc = ranks / (len(scores) - 1)\n",
    "    return perc\n",
    "\n",
    "def jaccard_topk(setA, setB):\n",
    "    if len(setA) == 0 and len(setB) == 0:\n",
    "        return 1.0\n",
    "    inter = len(setA.intersection(setB))\n",
    "    union = len(setA.union(setB))\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def main():\n",
    "    X, y_true = load_features(FEATURES_CSV)\n",
    "    n = X.shape[0]\n",
    "    print(f\"Loaded features: {X.shape[0]} users, {X.shape[1]} features\")\n",
    "    # determine contamination\n",
    "    global CONTAMINATION, TOP_K_RULE\n",
    "    if CONTAMINATION is None:\n",
    "        if y_true is not None:\n",
    "            CONTAMINATION = float(max(0.001, y_true.mean()))  # use empirical rate\n",
    "            print(f\"Inferred contamination from labels: {CONTAMINATION:.4f}\")\n",
    "        else:\n",
    "            CONTAMINATION = 0.02\n",
    "            print(f\"No labels found; using default contamination = {CONTAMINATION:.4f}\")\n",
    "    if TOP_K_RULE is None:\n",
    "        TOP_K_RULE = max(1, int(round(CONTAMINATION * n)))\n",
    "        print(f\"Top-K (for overlap) set to {TOP_K_RULE} (contamination*N)\")\n",
    "\n",
    "    seed_results = {}\n",
    "    all_percentiles = {}\n",
    "    all_scores = {}\n",
    "\n",
    "    # Train with multiple seeds and collect scores & predictions\n",
    "    for seed in SEEDS:\n",
    "        scores, y_pred, model, scaler = train_iforest_get_scores(X, seed, CONTAMINATION)\n",
    "        perc = percentile_from_scores(scores)  # 0..1\n",
    "        # we want 1 = most anomalous; currently perc low -> less anomalous because rank ascending. Fix:\n",
    "        # Because scores higher => more anomalous (we used -decision_function), ranks ascending -> small rank means least anomalous.\n",
    "        # Let's convert so perc_anom = 1 - perc\n",
    "        perc_anom = 1.0 - perc\n",
    "        seed_results[seed] = {\"scores\": scores, \"percentile\": perc_anom, \"y_pred\": y_pred}\n",
    "        all_percentiles[seed] = perc_anom\n",
    "        all_scores[seed] = scores\n",
    "        # save per-seed CSV\n",
    "        df_seed = pd.DataFrame({\n",
    "            \"user\": X.index,\n",
    "            \"anomaly_score_raw\": scores,\n",
    "            \"anomaly_percentile\": perc_anom,\n",
    "            \"predicted\": y_pred\n",
    "        }).set_index(\"user\")\n",
    "        df_seed.to_csv(os.path.join(OUT_DIR, f\"seed_{seed}_scores.csv\"))\n",
    "\n",
    "    # Stability analysis: Spearman rank corr between seeds, and top-K Jaccard\n",
    "    pairs = list(combinations(SEEDS, 2))\n",
    "    spearman_vals = []\n",
    "    jaccard_vals = []\n",
    "    for a, b in pairs:\n",
    "        sa = all_scores[a]\n",
    "        sb = all_scores[b]\n",
    "        # Spearman between raw anomaly scores\n",
    "        rho, p = spearmanr(sa, sb)\n",
    "        spearman_vals.append(rho if not np.isnan(rho) else 0.0)\n",
    "        # top-K sets\n",
    "        topA = set(X.index[np.argsort(-sa)][:TOP_K_RULE])  # sort descending -> most anomalous first\n",
    "        topB = set(X.index[np.argsort(-sb)][:TOP_K_RULE])\n",
    "        jacc = jaccard_topk(topA, topB)\n",
    "        jaccard_vals.append(jacc)\n",
    "    mean_spearman = np.mean(spearman_vals) if spearman_vals else 1.0\n",
    "    mean_jaccard = np.mean(jaccard_vals) if jaccard_vals else 1.0\n",
    "\n",
    "    # Ensemble aggregation: average percentiles across seeds\n",
    "    perc_matrix = np.vstack([all_percentiles[s] for s in SEEDS]).T  # n x seeds\n",
    "    ensemble_percentile = perc_matrix.mean(axis=1)\n",
    "    ensemble_scores_df = pd.DataFrame({\n",
    "        \"user\": X.index,\n",
    "        \"ensemble_percentile\": ensemble_percentile\n",
    "    }).set_index(\"user\")\n",
    "    ensemble_scores_df.to_csv(os.path.join(OUT_DIR, \"ensemble_percentile.csv\"))\n",
    "\n",
    "    # If labels available, evaluate varying percentile thresholds to choose an operating point\n",
    "    best_thresh = None\n",
    "    best_f1 = -1\n",
    "    threshold_grid = np.linspace(0.01, 0.99, 99)\n",
    "    metrics = []\n",
    "    if y_true is not None:\n",
    "        y = y_true.loc[X.index].astype(int).values\n",
    "        # compute AUC on ensemble_percentile\n",
    "        try:\n",
    "            auc = roc_auc_score(y, ensemble_percentile)\n",
    "        except Exception:\n",
    "            auc = None\n",
    "        for t in threshold_grid:\n",
    "            yhat = (ensemble_percentile >= t).astype(int)\n",
    "            p, r, f, _ = precision_recall_fscore_support(y, yhat, average='binary', zero_division=0)\n",
    "            metrics.append((t, p, r, f))\n",
    "            if f > best_f1:\n",
    "                best_f1 = f\n",
    "                best_thresh = t\n",
    "        # save calibration table\n",
    "        calib_df = pd.DataFrame(metrics, columns=[\"threshold\", \"precision\", \"recall\", \"f1\"]).set_index(\"threshold\")\n",
    "        calib_df.to_csv(os.path.join(OUT_DIR, \"threshold_calibration.csv\"))\n",
    "    else:\n",
    "        auc = None\n",
    "\n",
    "    # Save summary CSV with per-user info from ensemble and per-seed raw scores\n",
    "    summary = X.copy()\n",
    "    summary.index = X.index\n",
    "    for seed in SEEDS:\n",
    "        summary[f\"score_s{seed}\"] = all_scores[seed]\n",
    "        summary[f\"perc_s{seed}\"] = all_percentiles[seed]\n",
    "        summary[f\"pred_s{seed}\"] = seed_results[seed][\"y_pred\"]\n",
    "    summary[\"ensemble_percentile\"] = ensemble_percentile\n",
    "    if y_true is not None:\n",
    "        summary[\"true_attack\"] = y_true\n",
    "    summary.to_csv(os.path.join(OUT_DIR, \"stability_summary.csv\"))\n",
    "\n",
    "    # Print short human summary and recommendations\n",
    "    print(\"=== Stability & Calibration Summary ===\")\n",
    "    print(f\"Seeds used: {SEEDS}\")\n",
    "    print(f\"Samples: {n}, features: {X.shape[1]}\")\n",
    "    print(f\"Contamination used: {CONTAMINATION:.4f}, Top-K for overlap: {TOP_K_RULE}\")\n",
    "    print(f\"Mean Spearman rank correlation across seeds: {mean_spearman:.4f}\")\n",
    "    print(f\"Mean Top-{TOP_K_RULE} Jaccard overlap across seeds: {mean_jaccard:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"Ensemble ROC-AUC (using true labels): {auc:.4f}\")\n",
    "    if best_thresh is not None:\n",
    "        # compute metrics at best threshold\n",
    "        best_row = calib_df.loc[best_thresh]\n",
    "        print(f\"Best F1 threshold (ensemble percentile): {best_thresh:.3f} -> precision={best_row['precision']:.3f}, recall={best_row['recall']:.3f}, f1={best_row['f1']:.3f}\")\n",
    "        print(f\"Calibration table saved: {os.path.join(OUT_DIR, 'threshold_calibration.csv')}\")\n",
    "    print(f\"Per-user stability summary saved: {os.path.join(OUT_DIR, 'stability_summary.csv')}\")\n",
    "    print(f\"Per-seed CSVs and ensemble file saved in {OUT_DIR}/\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    if mean_spearman < 0.7:\n",
    "        print(\"- Ranking is unstable (Spearman < 0.7). Consider: increase n_estimators, train with more seeds and ensemble scores, or refine features.\")\n",
    "    else:\n",
    "        print(\"- Ranking is relatively stable across seeds.\")\n",
    "    if mean_jaccard < 0.5:\n",
    "        print(\"- Top-K overlap low. Consider: ensemble over seeds (we produced ensemble_percentile.csv), or increase signal via features.\")\n",
    "    if best_thresh is not None and best_f1 < 0.4:\n",
    "        print(\"- Model F1 is low at best threshold. Consider: adding features, ensembling with Autoencoder, or adding rule signals.\")\n",
    "    print(\"\\nNext recommended actions:\")\n",
    "    print(\"1) Inspect 'stability_summary.csv' and 'ensemble_percentile.csv' to see which users are top-ranked.\")\n",
    "    print(\"2) Use ensemble_percentile as the canonical anomaly score for downstream risk aggregation.\")\n",
    "    print(\"3) If ranking unstable, run Autoencoder as an orthogonal detector and ensemble its percentiles with IF (average).\")\n",
    "    print(\"4) Use the calibration table to pick an operational percentile threshold for alerts (or to map percentile -> risk bands).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb3a52-fcdf-4353-8225-bb4665648e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
