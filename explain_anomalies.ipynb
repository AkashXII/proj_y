{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e365034-6e09-42a6-800d-c2ce6b243ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features: ['avg_login_hour', 'std_login_hour', 'unique_countries', 'unique_devices', 'failed_login_rate', 'resource_entropy']\n",
      "Wrote explanations to explanations.csv\n",
      "\n",
      "--- Analyst-friendly report (top lines) ---\n",
      "                   avg_login_hour  std_login_hour  unique_countries  \\\n",
      "user                                                                  \n",
      "user1@example.com        7.023256        1.299970                 2   \n",
      "user4@example.com       17.023256        2.815674                 1   \n",
      "user3@example.com        6.952381        0.935802                 1   \n",
      "user5@example.com       12.095238        0.957882                 1   \n",
      "user8@example.com       16.952381        0.730933                 1   \n",
      "user7@example.com       17.142857        0.751305                 1   \n",
      "user2@example.com        8.261905        0.912235                 1   \n",
      "user6@example.com        7.404762        0.989198                 1   \n",
      "\n",
      "                   unique_devices  failed_login_rate  resource_entropy  \\\n",
      "user                                                                     \n",
      "user1@example.com               2           0.000000          1.780865   \n",
      "user4@example.com               1           0.046512          1.708759   \n",
      "user3@example.com               1           0.071429          1.710897   \n",
      "user5@example.com               1           0.047619          1.750324   \n",
      "user8@example.com               1           0.000000          1.749321   \n",
      "user7@example.com               1           0.000000          1.714711   \n",
      "user2@example.com               1           0.000000          1.734143   \n",
      "user6@example.com               1           0.023810          1.727700   \n",
      "\n",
      "                   anomaly_score  predicted_attack  true_attack  \\\n",
      "user                                                              \n",
      "user1@example.com      -0.094425                 1            1   \n",
      "user4@example.com      -0.003710                 1            1   \n",
      "user3@example.com       0.070483                 0            0   \n",
      "user5@example.com       0.105011                 0            0   \n",
      "user8@example.com       0.110357                 0            0   \n",
      "user7@example.com       0.115024                 0            0   \n",
      "user2@example.com       0.136925                 0            0   \n",
      "user6@example.com       0.136934                 0            0   \n",
      "\n",
      "                                                                                 top_features  \\\n",
      "user                                                                                            \n",
      "user1@example.com     unique_countries(0.087); unique_devices(0.082); resource_entropy(0.052)   \n",
      "user4@example.com       std_login_hour(0.098); resource_entropy(0.016); avg_login_hour(0.015)   \n",
      "user3@example.com  failed_login_rate(0.051); resource_entropy(0.010); unique_countries(0.000)   \n",
      "user5@example.com    resource_entropy(0.014); failed_login_rate(0.013); avg_login_hour(0.007)   \n",
      "user8@example.com       resource_entropy(0.013); avg_login_hour(0.011); std_login_hour(0.003)   \n",
      "user7@example.com       resource_entropy(0.008); avg_login_hour(0.005); std_login_hour(0.004)   \n",
      "user2@example.com      std_login_hour(0.001); avg_login_hour(0.001); failed_login_rate(0.000)   \n",
      "user6@example.com     std_login_hour(0.006); resource_entropy(0.001); unique_countries(0.000)   \n",
      "\n",
      "                                                                                                                                                                                                               explanation  \n",
      "user                                                                                                                                                                                                                        \n",
      "user1@example.com  unique_countries: contribution=0.0868, z_score=2.65 (strong deviation) | unique_devices: contribution=0.0818, z_score=2.65 (strong deviation) | resource_entropy: contribution=0.0517, z_score=2.00 ...  \n",
      "user4@example.com  std_login_hour: contribution=0.0977, z_score=2.56 (strong deviation) | resource_entropy: contribution=0.0155, z_score=-1.12 (moderate deviation) | avg_login_hour: contribution=0.0152, z_score=1.21...  \n",
      "user3@example.com  failed_login_rate: contribution=0.0514, z_score=1.80 (moderate deviation) | resource_entropy: contribution=0.0098, z_score=-1.02 (moderate deviation) | unique_countries: contribution=0.0000, z_sco...  \n",
      "user5@example.com                                           resource_entropy: contribution=0.0143, z_score=0.68 | failed_login_rate: contribution=0.0129, z_score=0.90 | avg_login_hour: contribution=0.0070, z_score=0.11  \n",
      "user8@example.com                        resource_entropy: contribution=0.0127, z_score=0.64 | avg_login_hour: contribution=0.0105, z_score=1.19 (moderate deviation) | std_login_hour: contribution=0.0026, z_score=-0.69  \n",
      "user7@example.com                       resource_entropy: contribution=0.0082, z_score=-0.86 | avg_login_hour: contribution=0.0053, z_score=1.24 (moderate deviation) | std_login_hour: contribution=0.0035, z_score=-0.66  \n",
      "user2@example.com                                          std_login_hour: contribution=0.0015, z_score=-0.41 | avg_login_hour: contribution=0.0011, z_score=-0.75 | failed_login_rate: contribution=0.0003, z_score=-0.89  \n",
      "user6@example.com                                         std_login_hour: contribution=0.0056, z_score=-0.29 | resource_entropy: contribution=0.0005, z_score=-0.30 | unique_countries: contribution=0.0000, z_score=-0.38  \n",
      "\n",
      "For each user, explanations.csv contains top contributing features and z-scores.\n",
      "Interpretation rule: positive contribution means that feature pushed the score towards anomaly for that user.\n",
      "Z-score indicates how far the user's value is from population mean (|z| >= 2 is strong).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\LEGION\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\LEGION\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\LEGION\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\LEGION\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\LEGION\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# explain_anomalies.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG\n",
    "FEATURES_CSV = \"features.csv\"   # output from your feature_engineering step\n",
    "OUTPUT_EXPLANATIONS = \"explanations.csv\"\n",
    "RANDOM_STATE = 42\n",
    "CONTAMINATION = 0.15   # keep consistent with your model earlier\n",
    "TOP_K_FEATURES = 3\n",
    "# --------------------------\n",
    "\n",
    "def load_features(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    # assume 'is_attack' exists only for evaluation; keep it separately\n",
    "    if \"is_attack\" in df.columns:\n",
    "        y_true = df[\"is_attack\"].astype(int)\n",
    "        X = df.drop(columns=[\"is_attack\"])\n",
    "    else:\n",
    "        y_true = None\n",
    "        X = df\n",
    "    return X, y_true\n",
    "\n",
    "def train_model(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    model = IsolationForest(\n",
    "        n_estimators=200,\n",
    "        contamination=CONTAMINATION,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    model.fit(X_scaled)\n",
    "    return model, scaler, X_scaled\n",
    "\n",
    "def compute_scores(model, scaler, X):\n",
    "    X_scaled = scaler.transform(X)\n",
    "    # decision_function: larger => more normal, smaller => more anomalous\n",
    "    scores = model.decision_function(X_scaled)\n",
    "    preds = model.predict(X_scaled)  # -1 anomaly, 1 normal\n",
    "    # convert to 1 (attack) / 0 (normal)\n",
    "    y_pred = np.where(preds == -1, 1, 0)\n",
    "    return scores, y_pred, X_scaled\n",
    "\n",
    "def feature_ablation_contributions(model, scaler, X, X_scaled, feature_names):\n",
    "    \"\"\"\n",
    "    For each feature and each user, set feature to population median and recompute score.\n",
    "    Contribution = (masked_score - original_score).\n",
    "    If contribution > 0, masking the feature made the point MORE normal => that feature was pushing it towards anomalous.\n",
    "    \"\"\"\n",
    "    # original scores\n",
    "    original_scores = model.decision_function(X_scaled)\n",
    "    medians = X.median(axis=0).values\n",
    "    contributions = np.zeros((X.shape[0], X.shape[1]))\n",
    "    # iterate features\n",
    "    for j, fname in enumerate(feature_names):\n",
    "        X_masked = X.copy().values\n",
    "        # set j-th column to median\n",
    "        X_masked[:, j] = medians[j]\n",
    "        X_masked_scaled = scaler.transform(X_masked)\n",
    "        masked_scores = model.decision_function(X_masked_scaled)\n",
    "        # contribution: masked - original (positive means feature contributed to anomaly)\n",
    "        contributions[:, j] = masked_scores - original_scores\n",
    "    return original_scores, contributions\n",
    "\n",
    "def make_explanations(X, scores, y_pred, contributions, feature_names, y_true=None, top_k=3):\n",
    "    rows = []\n",
    "    # population z-scores for context (per feature)\n",
    "    z = pd.DataFrame(zscore(X, nan_policy='omit'), columns=feature_names, index=X.index)\n",
    "    for i, uid in enumerate(X.index):\n",
    "        row = {}\n",
    "        row[\"user\"] = uid\n",
    "        row[\"anomaly_score\"] = float(scores[i])  # higher is more normal\n",
    "        row[\"predicted_attack\"] = int(y_pred[i])\n",
    "        if y_true is not None:\n",
    "            row[\"true_attack\"] = int(y_true.loc[uid])\n",
    "        # per-feature contributions for this user\n",
    "        contribs = contributions[i, :]\n",
    "        # make a sorted list of (feature, contribution, zscore)\n",
    "        feats = []\n",
    "        for j, fname in enumerate(feature_names):\n",
    "            feats.append((fname, float(contribs[j]), float(z.loc[uid, fname])))\n",
    "        feats_sorted = sorted(feats, key=lambda x: x[1], reverse=True)  # largest positive contribution first\n",
    "        top_feats = feats_sorted[:top_k]\n",
    "        # human readable explanation\n",
    "        explanation_lines = []\n",
    "        for fname, contrib, zscore_val in top_feats:\n",
    "            # contribution sign meaning:\n",
    "            # - positive: setting that feature to population median made user more normal => feature pushed towards anomalous\n",
    "            # Build explanation\n",
    "            expl = f\"{fname}: contribution={contrib:.4f}, z_score={zscore_val:.2f}\"\n",
    "            # add human context\n",
    "            if abs(zscore_val) >= 2.0:\n",
    "                expl += \" (strong deviation)\"\n",
    "            elif abs(zscore_val) >= 1.0:\n",
    "                expl += \" (moderate deviation)\"\n",
    "            explanation_lines.append(expl)\n",
    "        row[\"top_features\"] = \"; \".join([f\"{f[0]}({f[1]:.3f})\" for f in top_feats])\n",
    "        row[\"explanation\"] = \" | \".join(explanation_lines)\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows).set_index(\"user\")\n",
    "\n",
    "def main():\n",
    "    # 1. load\n",
    "    X, y_true = load_features(FEATURES_CSV)\n",
    "    feature_names = list(X.columns)\n",
    "    print(\"Loaded features:\", feature_names)\n",
    "    # 2. train an IsolationForest (reproducible)\n",
    "    model, scaler, X_scaled = train_model(X)\n",
    "    # 3. compute scores & predictions\n",
    "    scores, y_pred, X_scaled = compute_scores(model, scaler, X)\n",
    "    # 4. ablation contributions\n",
    "    original_scores, contributions = feature_ablation_contributions(model, scaler, X, X_scaled, feature_names)\n",
    "    # 5. explanations dataframe\n",
    "    explanations = make_explanations(X, original_scores, y_pred, contributions, feature_names, y_true=y_true, top_k=TOP_K_FEATURES)\n",
    "    # 6. combine for easy inspection\n",
    "    combined = X.copy()\n",
    "    combined[\"anomaly_score\"] = explanations[\"anomaly_score\"]\n",
    "    combined[\"predicted_attack\"] = explanations[\"predicted_attack\"]\n",
    "    if y_true is not None:\n",
    "        combined[\"true_attack\"] = explanations[\"true_attack\"]\n",
    "    combined[\"top_features\"] = explanations[\"top_features\"]\n",
    "    combined[\"explanation\"] = explanations[\"explanation\"]\n",
    "    # save\n",
    "    combined.to_csv(OUTPUT_EXPLANATIONS)\n",
    "    print(\"Wrote explanations to\", OUTPUT_EXPLANATIONS)\n",
    "    # pretty print summary\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    print(\"\\n--- Analyst-friendly report (top lines) ---\")\n",
    "    print(combined.sort_values(\"anomaly_score\").head(10))  # show most anomalous (lowest score)\n",
    "    print(\"\\nFor each user, explanations.csv contains top contributing features and z-scores.\")\n",
    "    print(\"Interpretation rule: positive contribution means that feature pushed the score towards anomaly for that user.\")\n",
    "    print(\"Z-score indicates how far the user's value is from population mean (|z| >= 2 is strong).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf58096-86ec-4865-b289-4b54d3036021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
